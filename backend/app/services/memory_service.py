from sqlalchemy.orm import Session
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
import json
import re
from ..models.models import Memory, WorkingMemory, User
from ..models.schemas import MemoryCreate, MemoryResponse, WorkingMemoryUpdate, MemorySearchRequest
from ..services.vector_service import vector_service
from ..services.llm_service import get_llm_service
try:
    import nltk
    from nltk.tokenize import sent_tokenize
    from nltk.corpus import stopwords
    NLTK_AVAILABLE = True
    # ä¸‹è½½NLTKæ•°æ®ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ï¼‰
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        try:
            nltk.download('punkt', quiet=True)
        except:
            pass
except ImportError:
    NLTK_AVAILABLE = False
    print("âš ï¸  nltkæœªå®‰è£…ï¼ŒæŸäº›æ–‡æœ¬å¤„ç†åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ: pip install nltk")
    sent_tokenize = None
    stopwords = None
from collections import Counter
import asyncio

if NLTK_AVAILABLE:
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        try:
            nltk.download('stopwords', quiet=True)
        except:
            pass

class MemoryService:
    def __init__(self, db: Session):
        self.db = db
        if NLTK_AVAILABLE and stopwords:
            try:
                self.stop_words = set(stopwords.words('english'))
            except:
                self.stop_words = set()
        else:
            self.stop_words = set()

    async def create_memory(self, memory_data: MemoryCreate) -> MemoryResponse:
        """åˆ›å»ºæ–°çš„è®°å¿†"""
        try:
            # æå–å…³é”®è¯å’Œé‡è¦æ€§åˆ†æ•°
            keywords, auto_importance_score = self._extract_keywords_and_importance(memory_data.content)

            # ä½¿ç”¨ä¼ å…¥çš„importance_scoreï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ä½¿ç”¨è‡ªåŠ¨è®¡ç®—çš„
            final_importance_score = memory_data.importance_score if memory_data.importance_score > 0 else auto_importance_score

            print(f"\n{'='*80}")
            print(f"ğŸ“ åˆ›å»ºæ–°è®°å¿†")
            print(f"{'='*80}")
            print(f"å†…å®¹: {memory_data.content[:100]}{'...' if len(memory_data.content) > 100 else ''}")
            print(f"ç±»å‹: {memory_data.memory_type}")
            print(f"ä¼ å…¥é‡è¦æ€§: {memory_data.importance_score}")
            print(f"è‡ªåŠ¨è®¡ç®—é‡è¦æ€§: {auto_importance_score:.2f}")
            print(f"æœ€ç»ˆé‡è¦æ€§: {final_importance_score:.2f}")
            print(f"æå–çš„å…³é”®è¯: {keywords}")
            print(f"æ ‡ç­¾: {memory_data.tags or keywords}")
            print(f"å…ƒæ•°æ®: {memory_data.metadata}")
            print(f"{'='*80}\n")

            # åˆ›å»ºè®°å¿†è®°å½•
            memory = Memory(
                content=memory_data.content,
                memory_type=memory_data.memory_type,
                importance_score=final_importance_score,
                user_id=memory_data.user_id,
                meta_data=memory_data.metadata or {},
                tags=memory_data.tags or keywords
            )

            self.db.add(memory)
            self.db.commit()
            self.db.refresh(memory)

            print(f"âœ… è®°å¿†å·²ä¿å­˜ - ID: {memory.id}\n")

            # æ·»åŠ åˆ°å‘é‡ç´¢å¼•
            await asyncio.to_thread(
                vector_service.add_memory_embedding,
                memory.id, memory.content, self.db
            )
            
            # æ‰‹åŠ¨æ„é€ å“åº”ï¼Œå› ä¸º ORM å­—æ®µæ˜¯ meta_data ä½† Pydantic æœŸæœ› metadata
            return MemoryResponse(
                id=memory.id,
                content=memory.content,
                memory_type=memory.memory_type,
                importance_score=memory.importance_score,
                access_count=memory.access_count,
                last_accessed=memory.last_accessed,
                created_at=memory.created_at,
                metadata=memory.meta_data or {},
                tags=memory.tags or []
            )

        except Exception as e:
            self.db.rollback()
            raise Exception(f"Failed to create memory: {e}")

    def search_memories(self, search_request: MemorySearchRequest) -> List[MemoryResponse]:
        """
        æ··åˆæœç´¢è®°å¿†ï¼ˆå‘é‡æœç´¢ + å…³é”®è¯åŒ¹é…ï¼‰
        """
        try:
            print(f"\n{'='*80}")
            print(f"ğŸ” æ··åˆæœç´¢è®°å¿† (Hybrid Search)")
            print(f"{'='*80}")
            print(f"æŸ¥è¯¢: {search_request.query[:80]}{'...' if len(search_request.query) > 80 else ''}")
            print(f"ç±»å‹è¿‡æ»¤: {search_request.memory_type or 'å…¨éƒ¨'}")
            print(f"ç”¨æˆ·ID: {search_request.user_id or 'å…¨éƒ¨'}")
            print(f"é™åˆ¶æ•°é‡: {search_request.limit}")
            print(f"ç›¸ä¼¼åº¦é˜ˆå€¼: {search_request.threshold}")
            print(f"{'-'*80}")
            
            # æ­¥éª¤1: ä»æŸ¥è¯¢ä¸­æå–å®ä½“å’Œå…³é”®è¯
            query_entities = self._extract_shopping_entities(search_request.query)
            
            # æ‰“å°æå–çš„å®ä½“
            has_entities = any(len(v) > 0 for v in query_entities.values())
            if has_entities:
                print(f"\nğŸ“‹ æå–çš„æŸ¥è¯¢å®ä½“:")
                for entity_type, entity_list in query_entities.items():
                    if entity_list:
                        print(f"   {entity_type}: {entity_list[:5]}")  # åªæ˜¾ç¤ºå‰5ä¸ª
            else:
                print(f"\nğŸ“‹ æœªæå–åˆ°æ˜ç¡®å®ä½“ï¼Œä½¿ç”¨çº¯å‘é‡æœç´¢")
            
            # æ­¥éª¤2: ä½¿ç”¨å‘é‡æœç´¢è·å–å€™é€‰é›†
            similar_memories = vector_service.search_similar_memories(
                query=search_request.query,
                limit=search_request.limit * 3,  # è·å–æ›´å¤šç»“æœç”¨äºå…³é”®è¯é‡æ’åº
                threshold=max(0.3, search_request.threshold - 0.2),  # é™ä½é˜ˆå€¼ä»¥è·å–æ›´å¤šå€™é€‰
                user_id=search_request.user_id,
                memory_type=search_request.memory_type,
                db=self.db
            )

            print(f"\nğŸ”¢ å‘é‡æœç´¢è¿”å›: {len(similar_memories)} æ¡å€™é€‰ç»“æœ")
            
            # æ­¥éª¤3: å¯¹æ¯æ¡è®°å¿†è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°ï¼Œå¹¶é‡æ–°æ’åº
            from ..models.models import Memory
            scored_memories = []
            
            for mem_data in similar_memories:
                # ä»æ•°æ®åº“è·å–å®Œæ•´çš„è®°å¿†ä¿¡æ¯
                memory = self.db.query(Memory).filter(Memory.id == mem_data["id"]).first()
                if not memory:
                    continue
                
                # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
                keyword_score = 0.0
                if has_entities:
                    keyword_score = self._calculate_keyword_match_score(
                        mem_data["content"], 
                        query_entities
                    )
                
                # è·å–å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°
                vector_score = mem_data.get("score", 0.0)
                
                # è®¡ç®—æ··åˆåˆ†æ•°
                # å¦‚æœæœ‰æ˜ç¡®å®ä½“ï¼Œå…³é”®è¯åŒ¹é…æƒé‡æ›´é«˜ï¼›å¦åˆ™ä¸»è¦ä¾èµ–å‘é‡æœç´¢
                if has_entities:
                    # æœ‰å®ä½“ï¼š60% å…³é”®è¯ + 40% å‘é‡
                    hybrid_score = 0.6 * keyword_score + 0.4 * vector_score
                    # å¦‚æœå…³é”®è¯å®Œå…¨ä¸åŒ¹é…ï¼Œå¤§å¹…é™ä½åˆ†æ•°
                    if keyword_score < 0.1:
                        hybrid_score *= 0.3
                else:
                    # æ— å®ä½“ï¼š100% å‘é‡
                    hybrid_score = vector_score
                
                # è€ƒè™‘è®°å¿†çš„é‡è¦æ€§
                importance_boost = mem_data["importance_score"] * 0.1
                final_score = hybrid_score + importance_boost
                
                scored_memories.append({
                    "memory": memory,
                    "mem_data": mem_data,
                    "vector_score": vector_score,
                    "keyword_score": keyword_score,
                    "hybrid_score": hybrid_score,
                    "final_score": final_score
                })
            
            # æ­¥éª¤4: æŒ‰æ··åˆåˆ†æ•°é‡æ–°æ’åº
            scored_memories.sort(key=lambda x: x["final_score"], reverse=True)
            
            # æ­¥éª¤5: è½¬æ¢ä¸ºMemoryResponse
            results = []
            for i, scored_mem in enumerate(scored_memories[:search_request.limit], 1):
                memory = scored_mem["memory"]
                mem_data = scored_mem["mem_data"]
                
                memory_response = MemoryResponse(
                    id=mem_data["id"],
                    content=mem_data["content"],
                    memory_type=mem_data["memory_type"],
                    importance_score=mem_data["importance_score"],
                    access_count=getattr(memory, "access_count", 0),
                    last_accessed=getattr(memory, "last_accessed", memory.created_at),
                    created_at=datetime.fromisoformat(mem_data["created_at"]) if isinstance(mem_data["created_at"], str) else mem_data["created_at"],
                    metadata=mem_data.get("metadata", {}),
                    tags=memory.tags or []
                )
                results.append(memory_response)
                
                # æ‰“å°æ¯æ¡æœç´¢ç»“æœï¼ˆæ˜¾ç¤ºæ··åˆåˆ†æ•°ï¼‰
                if i <= 5:  # åªæ‰“å°å‰5æ¡
                    print(f"\n  [{i}] ID:{mem_data['id']}")
                    print(f"      å‘é‡:{scored_mem['vector_score']:.3f} | å…³é”®è¯:{scored_mem['keyword_score']:.3f} | æ··åˆ:{scored_mem['hybrid_score']:.3f} | æœ€ç»ˆ:{scored_mem['final_score']:.3f}")
                    print(f"      ç±»å‹:{mem_data['memory_type']} | é‡è¦æ€§:{mem_data['importance_score']:.2f} | æ ‡ç­¾:{memory.tags}")
                    print(f"      å†…å®¹:{mem_data['content'][:80]}{'...' if len(mem_data['content']) > 80 else ''}")

            if len(scored_memories) > 5:
                print(f"\n  ... è¿˜æœ‰ {len(scored_memories) - 5} æ¡ç»“æœæœªæ˜¾ç¤º")
            
            print(f"\nâœ… æ··åˆæœç´¢å®Œæˆï¼Œæœ€ç»ˆè¿”å› {len(results)} æ¡è®°å¿†")
            print(f"{'='*80}\n")

            return results

        except Exception as e:
            print(f"Error searching memories: {e}")
            import traceback
            traceback.print_exc()
            return []

    def get_working_memory(self, session_id: str) -> Optional[Dict[str, Any]]:
        """è·å–å·¥ä½œè®°å¿†"""
        try:
            working_memory = self.db.query(WorkingMemory).filter(
                WorkingMemory.session_id == session_id,
                WorkingMemory.is_active == True
            ).first()

            if working_memory:
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if working_memory.expires_at and working_memory.expires_at < datetime.utcnow():
                    working_memory.is_active = False
                    self.db.commit()
                    return None

                return {
                    "context_data": working_memory.context_data or {},
                    "short_term_memory": working_memory.short_term_memory or {},
                    "expires_at": working_memory.expires_at
                }

            return None

        except Exception as e:
            print(f"Error getting working memory: {e}")
            return None

    async def update_working_memory(self, update_data: WorkingMemoryUpdate) -> Dict[str, Any]:
        """æ›´æ–°å·¥ä½œè®°å¿†"""
        try:
            working_memory = self.db.query(WorkingMemory).filter(
                WorkingMemory.session_id == update_data.session_id
            ).first()

            if not working_memory:
                # åˆ›å»ºæ–°çš„å·¥ä½œè®°å¿†
                expires_at = None
                if update_data.expires_in:
                    expires_at = datetime.utcnow() + timedelta(seconds=update_data.expires_in)

                working_memory = WorkingMemory(
                    session_id=update_data.session_id,
                    context_data=update_data.context_data or {},
                    short_term_memory=update_data.short_term_memory or {},
                    expires_at=expires_at
                )
                self.db.add(working_memory)
            else:
                # æ›´æ–°ç°æœ‰å·¥ä½œè®°å¿†
                if update_data.context_data is not None:
                    current_context = working_memory.context_data or {}
                    current_context.update(update_data.context_data)
                    working_memory.context_data = current_context

                if update_data.short_term_memory is not None:
                    current_memory = working_memory.short_term_memory or {}
                    current_memory.update(update_data.short_term_memory)
                    working_memory.short_term_memory = current_memory

                if update_data.expires_in:
                    working_memory.expires_at = datetime.utcnow() + timedelta(seconds=update_data.expires_in)

                working_memory.is_active = True

            self.db.commit()
            self.db.refresh(working_memory)

            return {
                "context_data": working_memory.context_data or {},
                "short_term_memory": working_memory.short_term_memory or {},
                "expires_at": working_memory.expires_at
            }

        except Exception as e:
            self.db.rollback()
            raise Exception(f"Failed to update working memory: {e}")

    def clear_working_memory(self, session_id: str):
        """æ¸…é™¤å·¥ä½œè®°å¿†"""
        try:
            working_memory = self.db.query(WorkingMemory).filter(
                WorkingMemory.session_id == session_id
            ).first()

            if working_memory:
                working_memory.is_active = False
                self.db.commit()

        except Exception as e:
            print(f"Error clearing working memory: {e}")

    async def consolidate_memories(self, user_id: Optional[int] = None):
        """æ•´åˆè®°å¿†ï¼Œå°†å·¥ä½œè®°å¿†è½¬ç§»åˆ°é•¿æœŸè®°å¿†"""
        try:
            # è·å–æ‰€æœ‰æ´»è·ƒçš„å·¥ä½œè®°å¿†
            working_memories = self.db.query(WorkingMemory).filter(
                WorkingMemory.is_active == True
            ).all()

            for wm in working_memories:
                if user_id and wm.context_data and wm.context_data.get("user_id") != user_id:
                    continue

                # å°†é‡è¦çš„å·¥ä½œè®°å¿†è½¬ä¸ºé•¿æœŸè®°å¿†
                if wm.short_term_memory:
                    for key, value in wm.short_term_memory.items():
                        if self._is_important_memory(key, value):
                            memory_content = f"Context: {wm.context_data}\nKey Information: {key} - {value}"

                            memory_data = MemoryCreate(
                                content=memory_content,
                                memory_type="episodic",
                                importance_score=0.7,
                                metadata={"source": "working_memory", "session_id": wm.session_id}
                            )

                            await self.create_memory(memory_data)

                # æ¸…é™¤å·¥ä½œè®°å¿†
                wm.is_active = False

            self.db.commit()

        except Exception as e:
            print(f"Error consolidating memories: {e}")

    async def extract_and_store_conversation_memory(self, conversation_id: int, user_id: Optional[int] = None):
        """ä»å¯¹è¯ä¸­æå–å¹¶å­˜å‚¨é‡è¦ä¿¡æ¯åˆ°è®°å¿†"""
        try:
            from ..models.models import Message

            # è·å–å¯¹è¯çš„æ‰€æœ‰æ¶ˆæ¯
            messages = self.db.query(Message).filter(
                Message.conversation_id == conversation_id
            ).order_by(Message.created_at.asc()).all()

            if not messages:
                return

            # ä½¿ç”¨LLMæå–é‡è¦ä¿¡æ¯
            conversation_text = "\n".join([
                f"{msg.role}: {msg.content}" for msg in messages
            ])

            extraction_prompt = f"""
            Analyze the following conversation and extract important information that should be remembered.
            Focus on:
            - User preferences and interests
            - Important facts mentioned
            - Decisions made
            - Action items
            - Personal information shared

            Conversation:
            {conversation_text}

            Return a JSON object with extracted memories in the format:
            {{
                "memories": [
                    {{
                        "content": "specific fact or information",
                        "importance": 0.8,
                        "tags": ["preference", "fact"],
                        "type": "semantic"
                    }}
                ]
            }}
            """

            llm_service = get_llm_service()
            response = await llm_service.chat_completion([
                {"role": "system", "content": "You are a memory extraction expert. Extract important information from conversations."},
                {"role": "user", "content": extraction_prompt}
            ])

            try:
                # è§£æLLMå“åº”
                import json
                result = json.loads(response["content"])

                for mem_data in result.get("memories", []):
                    memory_create = MemoryCreate(
                        content=mem_data["content"],
                        memory_type=mem_data.get("type", "semantic"),
                        importance_score=mem_data.get("importance", 0.5),
                        tags=mem_data.get("tags", []),
                        user_id=user_id,
                        metadata={"source": "conversation", "conversation_id": conversation_id}
                    )

                    await self.create_memory(memory_create)

            except json.JSONDecodeError:
                print("Failed to parse memory extraction response")

        except Exception as e:
            print(f"Error extracting conversation memory: {e}")

    def _extract_shopping_entities(self, text: str) -> Dict[str, List[str]]:
        """
        æå–è´­ç‰©ç›¸å…³çš„å®ä½“è¯ï¼ˆé€šç”¨ç‰ˆæœ¬ï¼Œé€‚ç”¨äºæ‰€æœ‰å•†å“ç±»å‹ï¼‰
        è¿”å›: {
            "brands": [...],
            "price_numbers": [...],
            "sizes": [...],
            "colors": [...],
            "materials": [...],
            "features": [...],
            "key_nouns": [...]
        }
        """
        entities = {
            "brands": [],
            "price_numbers": [],
            "sizes": [],
            "colors": [],
            "materials": [],
            "features": [],
            "key_nouns": []
        }
        
        text_lower = text.lower()
        
        # 1. æå–ä»·æ ¼ç›¸å…³æ•°å­—ï¼ˆé€šç”¨ï¼‰
        price_patterns = [
            r'(\d+\.?\d*)\s*å…ƒ',
            r'(\d+\.?\d*)\s*å—',
            r'(\d+\.?\d*)\s*rmb',
            r'(\d+\.?\d*)\s*Â¥',
            r'\$\s*(\d+\.?\d*)',
            r'é¢„ç®—.*?(\d+)',
            r'(\d{3,})',  # 3ä½ä»¥ä¸Šæ•°å­—ï¼Œå¯èƒ½æ˜¯ä»·æ ¼
        ]
        for pattern in price_patterns:
            matches = re.findall(pattern, text_lower)
            entities["price_numbers"].extend(matches)
        
        # 2. æå–å°ºå¯¸/å°ºç ï¼ˆé€šç”¨ï¼‰
        size_patterns = [
            r'(\d+\.?\d*)\s*ç ',
            r'(\d+\.?\d*)\s*å·',
            r'([xs]{1,3}l{0,3})\s*ç ',  # XS, S, M, L, XL, XXL, XXXL
            r'å°ºå¯¸.*?(\d+)',
            r'(\d+)\s*[cmæ¯«ç±³mm]',  # å°ºå¯¸å•ä½
        ]
        for pattern in size_patterns:
            matches = re.findall(pattern, text_lower, re.IGNORECASE)
            entities["sizes"].extend(matches)
        
        # 3. æå–é¢œè‰²ï¼ˆé€šç”¨ï¼Œä¸­è‹±æ–‡ï¼‰
        common_colors = [
            "é»‘è‰²", "ç™½è‰²", "çº¢è‰²", "è“è‰²", "ç»¿è‰²", "é»„è‰²", "ç´«è‰²", "ç²‰è‰²", "æ©™è‰²", "ç°è‰²", "æ£•è‰²", "è¤è‰²",
            "black", "white", "red", "blue", "green", "yellow", "purple", "pink", "orange", "gray", "grey", "brown",
            "é“¶è‰²", "é‡‘è‰²", "ç±³è‰²", "å¡å…¶", "navy", "beige", "khaki"
        ]
        for color in common_colors:
            if color in text_lower:
                entities["colors"].append(color)
        
        # 4. æå–æè´¨ï¼ˆé€šç”¨ï¼‰
        common_materials = [
            "çš®é©", "çœŸçš®", "pu", "å¸†å¸ƒ", "ç½‘é¢", "æ£‰", "æ¶¤çº¶", "å°¼é¾™", "ç¾Šæ¯›", "ä¸ç»¸",
            "leather", "canvas", "cotton", "polyester", "nylon", "wool", "silk",
            "é‡‘å±", "å¡‘æ–™", "æœ¨è´¨", "ç»ç’ƒ", "é™¶ç“·", "ä¸é”ˆé’¢",
            "metal", "plastic", "wood", "glass", "ceramic", "stainless"
        ]
        for material in common_materials:
            if material in text_lower:
                entities["materials"].append(material)
        
        # 5. æå–å“ç‰Œè¯ï¼ˆé€šç”¨ï¼ŒåŒ…å«å¸¸è§å“ç‰Œï¼‰
        # è¿™é‡Œä½¿ç”¨é€šç”¨å“ç‰Œè¯†åˆ«ç­–ç•¥ï¼šå¤§å†™å¼€å¤´çš„è¿ç»­è¯ã€çŸ¥åå“ç‰Œ
        common_brands = [
            # è¿åŠ¨å“ç‰Œ
            "nike", "è€å…‹", "adidas", "é˜¿è¿ªè¾¾æ–¯", "é˜¿è¿ª", "puma", "å½ªé©¬", "new balance", "nb", "æ–°ç™¾ä¼¦",
            "asics", "äºšç‘Ÿå£«", "converse", "åŒ¡å¨", "vans", "ä¸‡æ–¯", "reebok", "é”æ­¥", "under armour", "ua",
            # æ—¶å°šå“ç‰Œ
            "gucci", "å¤é©°", "lv", "louis vuitton", "prada", "æ™®æ‹‰è¾¾", "chanel", "é¦™å¥ˆå„¿", "dior", "è¿ªå¥¥",
            "hermes", "çˆ±é©¬ä»•", "burberry", "å·´å®è‰", "coach", "è”»é©°",
            # ç”µå­å“ç‰Œ
            "apple", "è‹¹æœ", "samsung", "ä¸‰æ˜Ÿ", "huawei", "åä¸º", "xiaomi", "å°ç±³", "oppo", "vivo",
            "sony", "ç´¢å°¼", "dell", "æˆ´å°”", "lenovo", "è”æƒ³", "hp", "æƒ æ™®", "asus", "åç¡•",
            # å®¶å±…å“ç‰Œ
            "ikea", "å®œå®¶", "muji", "æ— å°è‰¯å“", "zara home", "h&m home",
            # ç¾å¦†å“ç‰Œ
            "loreal", "æ¬§è±é›…", "estee lauder", "é›…è¯—å…°é»›", "lancome", "å…°è”»", "dior", "chanel",
            # é£Ÿå“å“ç‰Œ
            "nestle", "é›€å·¢", "coca cola", "å¯å£å¯ä¹", "pepsi", "ç™¾äº‹", "starbucks", "æ˜Ÿå·´å…‹"
        ]
        for brand in common_brands:
            if brand in text_lower:
                entities["brands"].append(brand)
        
        # 6. æå–é‡è¦ç‰¹å¾è¯ï¼ˆé€šç”¨è´­ç‰©ç‰¹å¾ï¼‰
        feature_keywords = [
            "è½»ä¾¿", "èˆ’é€‚", "é€æ°”", "é˜²æ°´", "è€ç£¨", "é˜²æ»‘", "å‡éœ‡", "æ”¯æ’‘",
            "lightweight", "comfortable", "breathable", "waterproof", "durable", "non-slip",
            "æ™ºèƒ½", "é«˜ç«¯", "å…¥é—¨", "ä¸“ä¸š", "æ€§ä»·æ¯”", "ç»å…¸", "æ—¶å°š", "ç®€çº¦", "å¤å¤",
            "smart", "premium", "entry-level", "professional", "classic", "fashionable", "minimalist", "vintage",
            "æ— çº¿", "æœ‰çº¿", "è“ç‰™", "wifi", "4g", "5g",
            "wireless", "wired", "bluetooth"
        ]
        for feature in feature_keywords:
            if feature in text_lower:
                entities["features"].append(feature)
        
        # 7. æå–å…³é”®åè¯ï¼ˆä½¿ç”¨åˆ†è¯ï¼Œè¿‡æ»¤åœç”¨è¯ï¼‰
        words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', text)
        words = [w for w in words if len(w) > 1 and w.lower() not in self.stop_words]
        
        # ä½¿ç”¨è¯é¢‘ç»Ÿè®¡æå–å…³é”®è¯
        word_freq = Counter([w.lower() for w in words])
        entities["key_nouns"] = [word for word, freq in word_freq.most_common(10) if freq > 0]
        
        return entities

    def _calculate_keyword_match_score(self, text: str, entities: Dict[str, List[str]]) -> float:
        """
        è®¡ç®—æ–‡æœ¬ä¸å®ä½“çš„å…³é”®è¯åŒ¹é…åº¦
        è¿”å› 0.0-1.0 çš„åˆ†æ•°
        """
        text_lower = text.lower()
        total_matches = 0
        total_entities = 0
        
        # æƒé‡é…ç½®ï¼ˆä¸åŒç±»å‹çš„å®ä½“æœ‰ä¸åŒçš„é‡è¦æ€§ï¼‰
        weights = {
            "brands": 3.0,          # å“ç‰Œæœ€é‡è¦
            "price_numbers": 2.5,   # ä»·æ ¼æ•°å­—å¾ˆé‡è¦
            "sizes": 2.5,           # å°ºå¯¸å¾ˆé‡è¦
            "colors": 2.0,          # é¢œè‰²é‡è¦
            "materials": 1.5,       # æè´¨è¾ƒé‡è¦
            "features": 1.5,        # ç‰¹å¾è¾ƒé‡è¦
            "key_nouns": 1.0        # å…³é”®è¯åŸºç¡€æƒé‡
        }
        
        for entity_type, entity_list in entities.items():
            if not entity_list:
                continue
            
            weight = weights.get(entity_type, 1.0)
            
            for entity in entity_list:
                if not entity:
                    continue
                    
                entity_lower = str(entity).lower()
                total_entities += weight
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…
                if entity_lower in text_lower:
                    total_matches += weight
                # æ¨¡ç³ŠåŒ¹é…ï¼ˆé’ˆå¯¹æ•°å­—ï¼Œå…è®¸ä¸€å®šèŒƒå›´ï¼‰
                elif entity_type == "price_numbers":
                    try:
                        entity_num = float(entity)
                        # åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾ç›¸è¿‘çš„æ•°å­—
                        numbers_in_text = re.findall(r'\d+\.?\d*', text_lower)
                        for num_str in numbers_in_text:
                            text_num = float(num_str)
                            # å…è®¸10%çš„è¯¯å·®
                            if abs(text_num - entity_num) / max(entity_num, 1) < 0.1:
                                total_matches += weight * 0.8  # æ¨¡ç³ŠåŒ¹é…å¾—åˆ†ç•¥ä½
                                break
                    except:
                        pass
        
        if total_entities == 0:
            return 0.0  # å¦‚æœæ²¡æœ‰æå–åˆ°ä»»ä½•å®ä½“ï¼Œè¿”å›0ï¼ˆè¯´æ˜æŸ¥è¯¢è¿‡äºæ³›åŒ–ï¼‰
        
        return min(1.0, total_matches / total_entities)

    def _extract_keywords_and_importance(self, text: str) -> Tuple[List[str], float]:
        """æå–å…³é”®è¯å’Œé‡è¦æ€§åˆ†æ•°"""
        try:
            # ç®€å•çš„å…³é”®è¯æå–
            words = re.findall(r'\b\w+\b', text.lower())
            words = [w for w in words if w not in self.stop_words and len(w) > 2]

            word_freq = Counter(words)
            keywords = [word for word, freq in word_freq.most_common(5)]

            # è®¡ç®—é‡è¦æ€§åˆ†æ•°ï¼ˆåŸºäºé•¿åº¦ã€å…³é”®è¯å¯†åº¦ç­‰ï¼‰
            importance_score = min(1.0, len(keywords) / 10.0)
            if any(keyword in text.lower() for keyword in ["important", "remember", "note", "key"]):
                importance_score += 0.2
            if "?" in text or "!" in text:
                importance_score += 0.1

            return keywords, min(1.0, importance_score)

        except Exception as e:
            print(f"Error extracting keywords: {e}")
            return [], 0.5

    def _is_important_memory(self, key: str, value: Any) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé‡è¦è®°å¿†"""
        important_keywords = [
            "preference", "important", "remember", "key", "decision",
            "goal", "objective", "plan", "strategy", "personal"
        ]

        text = f"{key} {str(value)}".lower()
        return any(keyword in text for keyword in important_keywords)

    async def get_relevant_context(self, query: str, session_id: str, user_id: Optional[int] = None, conversation_id: Optional[int] = None) -> Dict[str, Any]:
        """
        è·å–ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä½¿ç”¨æ··åˆæ£€ç´¢ + æ‰€æœ‰å†å²è®°å¿†ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            session_id: ä¼šè¯ID
            user_id: ç”¨æˆ·ID
            conversation_id: å¯¹è¯IDï¼ˆç”¨äºä¼˜å…ˆæ˜¾ç¤ºåŒä¸€å¯¹è¯çš„è®°å¿†ï¼‰
        """
        try:
            # æå–æŸ¥è¯¢ä¸­çš„å®ä½“ï¼Œç”¨äºåˆ¤æ–­æœç´¢ç­–ç•¥
            query_entities = self._extract_shopping_entities(query)
            has_specific_entities = any(len(v) > 0 for k, v in query_entities.items() if k in ["brands", "price_numbers", "sizes", "colors"])
            
            print(f"\n{'='*80}")
            print(f"ğŸ” å¼€å§‹æ£€ç´¢ç›¸å…³è®°å¿†")
            print(f"{'='*80}")
            print(f"Query: {query[:60]}...")
            if has_specific_entities:
                print(f"âœ“ æ£€æµ‹åˆ°æ˜ç¡®å®ä½“ï¼Œå°†ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰")
                for k, v in query_entities.items():
                    if v and k in ["brands", "price_numbers", "sizes", "colors"]:
                        print(f"   - {k}: {v[:3]}")
            else:
                print(f"âœ“ æœªæ£€æµ‹åˆ°æ˜ç¡®å®ä½“ï¼Œä½¿ç”¨çº¯å‘é‡æ£€ç´¢")
            print(f"{'-'*80}\n")
            
            # é¦–å…ˆæœç´¢åå¥½è®°å¿†ï¼ˆä½¿ç”¨æ··åˆæ£€ç´¢ + æ ‡ç­¾è¿‡æ»¤ï¼‰

            preference_memories = self.search_memories(MemorySearchRequest(
                query=query,  # ä½¿ç”¨åŸå§‹æŸ¥è¯¢
                limit=15,
                threshold=0.45 if has_specific_entities else 0.5,
                user_id=user_id,
                memory_type="semantic"  # åå¥½è®°å¿†é€šå¸¸æ˜¯è¯­ä¹‰è®°å¿†
            ))
            
            # è¿‡æ»¤å‡ºçœŸæ­£çš„åå¥½è®°å¿†ï¼ˆæœ‰ 'preference' æ ‡ç­¾çš„ï¼‰
            preference_memories = [
                mem for mem in preference_memories 
                if 'preference' in (mem.tags or []) or 'user_preference' in (mem.tags or [])
            ]
            
            # ç„¶åæœç´¢ä¸€èˆ¬è®°å¿†ï¼ˆä½¿ç”¨æ··åˆæ£€ç´¢ï¼‰
            general_memories = self.search_memories(MemorySearchRequest(
                query=query,
                limit=10,
                threshold=0.50 if has_specific_entities else 0.55,
                user_id=user_id
            ))
            
            # åˆå¹¶ç»“æœï¼Œä¼˜å…ˆæ˜¾ç¤ºåå¥½è®°å¿†å’ŒåŒä¸€å¯¹è¯çš„è®°å¿†
            all_memories = []
            memory_ids = set()
            
            # 1. ä¼˜å…ˆæ·»åŠ åŒä¸€å¯¹è¯çš„è®°å¿†
            if conversation_id:
                from ..models.models import Memory
                
                # æŸ¥è¯¢åŒä¸€å¯¹è¯çš„è®°å¿†ï¼ˆmetadataä¸­åŒ…å«conversation_idï¼‰
                all_conversation_memories = self.db.query(Memory).filter(
                    (Memory.user_id == user_id) if user_id else True
                ).all()
                
                conversation_memories = []
                for mem in all_conversation_memories:
                    if mem.meta_data and isinstance(mem.meta_data, dict):
                        if mem.meta_data.get("conversation_id") == conversation_id:
                            conversation_memories.append(mem)
                
                # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
                conversation_memories.sort(key=lambda x: x.created_at, reverse=True)
                
                for mem in conversation_memories[:10]:
                    if mem.id not in memory_ids:
                        all_memories.append({
                            "content": mem.content,
                            "type": mem.memory_type,
                            "importance": mem.importance_score,
                            "source": "same_conversation"
                        })
                        memory_ids.add(mem.id)
            
            # 2. æ·»åŠ åå¥½è®°å¿†ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰
            for mem in preference_memories:
                if mem.id not in memory_ids:
                    # æ£€æŸ¥æ˜¯å¦æœ‰åå¥½æ ‡ç­¾
                    has_preference_tag = "preference" in (mem.tags or [])
                    if has_preference_tag or mem.memory_type == "semantic":
                        all_memories.append({
                            "content": mem.content,
                            "type": mem.memory_type,
                            "importance": mem.importance_score,
                            "source": "preference_memory"
                        })
                        memory_ids.add(mem.id)
            
            # 3. æ·»åŠ ä¸€èˆ¬è®°å¿†
            for mem in general_memories:
                if mem.id not in memory_ids:
                    all_memories.append({
                        "content": mem.content,
                        "type": mem.memory_type,
                        "importance": mem.importance_score,
                        "source": "general_memory"
                    })
                    memory_ids.add(mem.id)
            
            print(f"ğŸ“š æ··åˆæ£€ç´¢ç»“æœï¼š{len(preference_memories)} æ¡åå¥½è®°å¿†ï¼Œ{len(general_memories)} æ¡ä¸€èˆ¬è®°å¿†")
            
            # æŒ‰é‡è¦æ€§æ’åºï¼Œé™åˆ¶æ•°é‡
            all_memories.sort(key=lambda x: x["importance"], reverse=True)
            relevant_memories = all_memories[:15]  # å¢åŠ åˆ°15æ¡ï¼Œæä¾›æ›´å¤šä¸Šä¸‹æ–‡

            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            print(f"\n{'='*60}")
            print(f"ğŸ§  æœ€ç»ˆè®°å¿†æ£€ç´¢ç»“æœ")
            print(f"{'='*60}")
            print(f"ğŸ“Š æ€»å…±æ£€ç´¢åˆ° {len(relevant_memories)} æ¡ç›¸å…³è®°å¿†:")
            for i, mem in enumerate(relevant_memories[:5], 1):  # æ˜¾ç¤ºå‰5æ¡
                print(f"\n[{i}] {mem['source']} (é‡è¦æ€§: {mem['importance']:.2f}, ç±»å‹: {mem['type']})")
                print(f"   å†…å®¹: {mem['content'][:100]}...")
            if len(relevant_memories) > 5:
                print(f"\n   ... è¿˜æœ‰ {len(relevant_memories) - 5} æ¡è®°å¿†æœªæ˜¾ç¤º")
            print(f"{'='*60}\n")

            # è·å–å·¥ä½œè®°å¿†
            working_memory = self.get_working_memory(session_id)

            # æ„å»ºä¸Šä¸‹æ–‡
            context = {
                "relevant_memories": relevant_memories,
                "working_memory": working_memory or {},
                "session_context": {
                    "session_id": session_id,
                    "conversation_id": conversation_id,
                    "timestamp": datetime.utcnow().isoformat()
                }
            }

            return context

        except Exception as e:
            print(f"Error getting relevant context: {e}")
            import traceback
            traceback.print_exc()
            return {
                "relevant_memories": [],
                "working_memory": {},
                "session_context": {
                    "session_id": session_id,
                    "timestamp": datetime.utcnow().isoformat()
                }
            }

    async def update_memory_access(self, memory_id: int):
        """æ›´æ–°è®°å¿†è®¿é—®è®°å½•"""
        try:
            memory = self.db.query(Memory).filter(Memory.id == memory_id).first()
            if memory:
                memory.access_count += 1
                memory.last_accessed = datetime.utcnow()
                self.db.commit()

        except Exception as e:
            print(f"Error updating memory access: {e}")

# å…¨å±€è®°å¿†æœåŠ¡å®ä¾‹ï¼ˆéœ€è¦é€šè¿‡ä¾èµ–æ³¨å…¥ä½¿ç”¨ï¼‰
def get_memory_service(db: Session) -> MemoryService:
    return MemoryService(db)